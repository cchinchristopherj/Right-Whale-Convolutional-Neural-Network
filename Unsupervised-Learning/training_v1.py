import numpy 
from skimage.transform import resize
from sklearn.metrics import roc_auc_score
from sklearn.cluster import MiniBatchKMeans
from sklearn.feature_extraction import image
from sklearn.decomposition import PCA
from sklearn.preprocessing import MinMaxScaler, normalize
from sklearn.metrics.pairwise import pairwise_distances
from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping
from keras import layers
from keras.layers import Input, Lambda, Dense, Reshape, Dropout, Activation, BatchNormalization, Flatten, Conv2D, MaxPooling2D, Concatenate 
from keras.models import load_model, Model
from keras.optimizers import Adam
import tensorflow as tf
import keras
import keras.backend as K
K.set_image_data_format('channels_last')

def create_model(num_groups,group_size,kernel_size,input_shape,connections_1f,connections_2f,connections_1m,connections_2m):
    ''' create_model Method
            CNN Model for Right Whale Upcall Recognition with filters learned through K-Means
            and energy-correlated receptive fields
            
            Args:
                num_groups: int number of groups to split the filters of the first and second
                            layers into 
                group_size: int size of the groups created in the first and second layers
                kernel_size: int size of the filters in the Conv2D layers (same kernel size
                             for all three layers)
                input_shape: (Number of Samples, Height, Width, Number of Filters)
                connections_1f: 2-D matrix of connections created by "learn_connections_unsup"
                                comprised of "num_groups" rows and "group_size" columns, for
                                the first layer
                connections_2f: 2-D matrix of connections created by "learn_connections_unsup"
                                for the second layer
                connections_1m: Mask in the shape of a representative feature map, to be
                                broadcasted across all feature maps in the first layer
                connections_2m: Mask in the shape of a representative feature map, to be 
                                broadcasted across all feature maps in the second layer
            Returns: 
                Compiled Keras CNN Model
                
    '''
    # There are three layers: Layer 0 is the convolution of the raw input layer with the
    # first set of learned filters (each filter is of depth 1 because the raw input layer 
    # is of depth 1). Layer 1 corresponds to the second set of learned filters (each filter is
    # of depth "group_size" and the learned filters are applied to smaller groups of the
    # entire set, each group being of depth "group_size"). Layer 2 corresponds to the third set 
    # of learned filters (each filter is of depth "group_size" and the learned filters are also 
    # applied to smaller groups of the entire set, each group being of depth "group_size").
    X_input = Input(shape=input_shape,name='input')
    # Dropout on the visible layer (1 in 5 probability of dropout) 
    X = Dropout(0.2,name='dropout0')(X_input)
    # BatchNorm on axis=3 (on the axis corresponding to the number of filters)
    X = BatchNormalization(axis=3,name='bn0')(X)
    # Convolution of Filters with Input. Since the filters are determined through unsupervised
    # learning, they are not updated through backpropagation (i.e. trainable=False)
    X = Conv2D(filters=num_groups*group_size,kernel_size=kernel_size,use_bias=False,activation='relu',name='conv0',trainable=False)(X)
    # Maxpooling for translation invariance and to halve height and width dimensions 
    X_maps1 = MaxPooling2D(name='maxpool0')(X)
    # connections_1m is a mask of 0s and 1s to multiply X_maps1 by. The mask was generated by
    # using a pairwise similarity metric (energy-correlation) to determine which parts of 
    # X_maps1 are most strongly correlated. (Parts that are not strongly correlated are 
    # replaced by zeros to aid training more discriminative filters via K-Means).
    X_maps1_masked = Lambda(lambda X: K.tf.multiply(X,connections_1m),name='masked1')(X_maps1)
    # The filters learned via K-Means for the next layer are trained on groups of feature maps
    # instead of the entire set. Specifically, the set of feature maps from X_maps1_masked
    # are broken up into "num_groups" groups of "group_size" filters, with the groups
    # determined via energy correlation. (The reduced dimensionality improves the performance 
    # of K-Means). These smaller groups of feature maps are then fed to K-Means to generate
    # new filters. Note, however, that it is not possible to apply the learned filters of 
    # reduced dimensionality immediately to the output of X_maps1_masked of full dimensionality.
    # In order to account for this discrepancy, the output of X_maps1_masked is split
    # into the same groups used for K-Means, and all the learned filters applied to each of
    # these smaller groups. For example, suppose X_maps1_masked originally has 64 feature maps, 
    # and these are split into 16 groups of 4 feature maps. After these groups are fed into 
    # K-Means and, say, 128 filters are generated, X_maps1_masked is split into the same 
    # 16 groups of 4 feature maps. Then, the 128 filters are applied to each of these 16 
    # groups (since each group is the same size as the groups originally fed to K-Means). The
    # results are then concatenated along the axis corresponding to the number of filters
    # (the last axis) to represent the output of the convolution. 
    # The dictionaries below are used to implement this grouping mechanism. Note that the 
    # connections_1f array contains all the groups of feature maps determined via 
    # energy-correlation. This array is used to slice the original set of feature maps into
    # the desired groups. 
    layers_1 = dict()
    # connections_1f is composed of "num_groups" rows and "group_size" columns
    # layers_1_lambda, layers_1_reshape, and layers_1_maxpool are dictionaries of 
    # sub-dictionaries, where each sub-dictionary corresponds to one of the "num_groups" groups 
    # in connections_1f
    layers_1_lambda = dict()
    layers_1_reshape = dict()
    layers_1_maxpool = dict()
    for ii in range(num_groups):
        # Instantiate the sub-dictionaries for each main dictionary, for the current group
        # under consideration
        layers_1[ii] = dict()
        layers_1_lambda[ii] = dict()
        layers_1_reshape[ii] = dict()
        layers_1_maxpool[ii] = dict()
        for jj in range(group_size):
            # For the current group under consideration (represented by ii) and the current
            # member of the group under consideration (represented by jj), use Keras'
            # Lambda layer to select that one member of the group (that one feature map) 
            # from the entire main group. Then use Keras' Reshape layer to reshape the 
            # dimensions of the output of the Lambda layer to:
            # (Number of samples, height, width, 1)
            layers_1_lambda[ii]['X_lambda_'+str(ii)+'_'+str(jj)] = Lambda(lambda X: X[:,:,:,connections_1f[ii,jj]],name='lambda1_'+str(ii)+'_'+str(jj))(X_maps1_masked)
            layers_1_reshape[ii]['X_reshape_'+str(ii)+'_'+str(jj)] = Reshape((*K.int_shape(layers_1_lambda[ii]['X_lambda_'+str(ii)+'_'+str(jj)])[1:3],1),name='reshape1_'+str(ii)+'_'+str(jj))(layers_1_lambda[ii]['X_lambda_'+str(ii)+'_'+str(jj)])
        # After the for loop above, layers_1_reshape[ii] contains all the individual feature
        # maps comprising the current group under consideration. Concatenate all of them into
        # one group of "group_size" feature maps using keras.layers.concatenate
        layers_1_concat = [layer for layer in layers_1_reshape[ii].values()]
        layers_1[ii]['X_concat_'+str(ii)] = keras.layers.concatenate(layers_1_concat,name='concat1_'+str(ii))
        # Apply BatchNorm along axis=3 as before
        layers_1[ii]['X_batchnorm_'+str(ii)] = BatchNormalization(axis=3,name='bn1_'+str(ii))(layers_1[ii]['X_concat_'+str(ii)])
        # Apply all the filters learned via K-Means to the current group of feature maps
        # under consideration. (Note that samples from all the groups were fed into K-Means to 
        # learn these filters (num_groups*group_size/8 filters were learned). These
        # num_groups*group_size/8 filters were applied to the current group under consideration.
        layers_1[ii]['X_conv2d_'+str(ii)] = Conv2D(filters=group_size,kernel_size=kernel_size,use_bias=False,activation='relu',name='conv1_'+str(ii),trainable=False)(layers_1[ii]['X_batchnorm_'+str(ii)])
        # Maxpooling was applied for translation invariance and to halve the width and height
        # dimensions
        layers_1_maxpool[ii]['X_maxpool2d_'+str(ii)] = MaxPooling2D(pool_size=(2,2),name='maxpool1_'+str(ii))(layers_1[ii]['X_conv2d_'+str(ii)])
    # Concatenate the results for all the groups into one set. This is done because 
    # connections_2f separates this entire set into energy-correlated groups (presumably 
    # different groups than specified by connections_1f).
    layers_1_final = []
    for ii in range(num_groups):
        # The sub-dictionaries in the layers_1_maxpool main dictionary contain all the
        # results to be concatenated
        layers_1_final.extend([layer for layer in layers_1_maxpool[ii].values()])
    X_maps2 = keras.layers.concatenate(layers_1_final,name='final1')
    # Repeat all the steps above for the third layer. Multiply X_maps2 by the mask represented
    # by connections_2m
    X_maps2_masked = Lambda(lambda X: K.tf.multiply(X,connections_2m),name='masked2')(X_maps2)
    # Generate the main dictionaries 
    layers_2 = dict()
    layers_2_lambda = dict()
    layers_2_reshape = dict()
    layers_2_maxpool = dict()
    for ii in range(num_groups):
        # Generate the sub-dictionaries
        layers_2[ii] = dict()
        layers_2_lambda[ii] = dict()
        layers_2_reshape[ii] = dict()
        layers_2_maxpool[ii] = dict()
        for jj in range(group_size):
            # For each group, compile each feature map member individually using the Lambda
            # and Reshape layers
            layers_2_lambda[ii]['X_lambda_'+str(ii)+'_'+str(jj)] = Lambda(lambda X: X[:,:,:,connections_2f[ii,jj]],name='lambda2_'+str(ii)+'_'+str(jj))(X_maps2_masked)
            layers_2_reshape[ii]['X_reshape_'+str(ii)+'_'+str(jj)] = Reshape((*K.int_shape(layers_2_lambda[ii]['X_lambda_'+str(ii)+'_'+str(jj)])[1:3],1),name='reshape2_'+str(ii)+'_'+str(jj))(layers_2_lambda[ii]['X_lambda_'+str(ii)+'_'+str(jj)])
        # Concatenate the feature maps for the group under consideration into one set
        layers_2_concat = [layer for layer in layers_2_reshape[ii].values()]
        layers_2[ii]['X_concat_'+str(ii)] = keras.layers.concatenate(layers_2_concat,name='concat2_'+str(ii))
        layers_2[ii]['X_batchnorm_'+str(ii)] = BatchNormalization(axis=3,name='bn2_'+str(ii))(layers_1[ii]['X_concat_'+str(ii)])
        # Apply the learned filters (num_groups*group_size/4 of them) to each of these 
        # smaller feature map groups
        layers_2[ii]['X_conv2d_'+str(ii)] = Conv2D(filters=group_size,kernel_size=kernel_size,use_bias=False,activation='relu',name='conv2_'+str(ii),trainable=False)(layers_2[ii]['X_batchnorm_'+str(ii)])
        layers_2_maxpool[ii]['X_maxpool2d_'+str(ii)] = MaxPooling2D(pool_size=(2,2),name='maxpool2_'+str(ii))(layers_2[ii]['X_conv2d_'+str(ii)])
    # Concatenate the results for all the groups into one set.
    layers_2_final = []
    for ii in range(num_groups):
        layers_2_final.extend([layer for layer in layers_2_maxpool[ii].values()])
    X_maps3 = keras.layers.concatenate(layers_2_final,name='final2')
    # Flatten the output from the first, second, and third layers
    X_maps1_f = Flatten(name='flatten1')(X_maps1)
    X_maps2_f = Flatten(name='flatten2')(X_maps2)
    X_maps3_f = Flatten(name='flatten3')(X_maps3)
    # Concatenate the flattened outputs into one feature vector
    X_maps = keras.layers.concatenate([X_maps1_f,X_maps2_f,X_maps3_f],name='final3')
    # Dropout on the fully connected layer (1 in 2 probability of dropout) 
    X = Dropout(0.5,name='dropout3')(X_maps)
    # Feed the feature vector into the Dense layer for binary classification
    X_output = Dense(1,activation='sigmoid',name='fc1')(X)
    # Use Adam optimizer
    opt = Adam(lr=0.0001,beta_1=0.9,beta_2=0.999,decay=0.01)
    model = Model(inputs=X_input,outputs=X_output)
    model.compile(optimizer=opt,loss='binary_crossentropy',metrics=['accuracy'])
    return model

def MiniBatchKMeansAutoConv(X, patch_size, max_patches, n_clusters, conv_orders, batch_size=20):
    ''' MiniBatchKMeansAutoConv Method 
            Extract patches from the input X, perform all specified orders of recursive
            autoconvolution to generate a richer set of patches, and pass the complete set
            of patches to MiniBatchKMeans to learn a dictionary of filters 
            
            Args:
                X: (Number of samples, Number of filters, Height, Width)
                patch_size: int size of patches to extract from X 
                max_patches: float decimal percentage of maximum number of patches to 
                             extract from X
                n_clusters: int number of centroids (filters) to learn via MiniBatchKMeans
                conv_orders: 1-D array of integers indicating which orders of recursive 
                             autoconvolution to perform, with the set of possible orders 
                             ranging from 0 to 3 inclusive
                batch_size: int size of batches to use in MiniBatchKMeans
            Returns: 
                learned centroids of shape: (Number of samples, Number of filters, Height, Width)
                
    '''
    sz = X.shape
    # Transpose to Shape: (Number of samples, Number of Filters, Height, Width) and extract
    # patches from each sample up to the maximum number of patches using sklearn's
    # PatchExtractor
    X = image.PatchExtractor(patch_size=patch_size,max_patches=max_patches).transform(X.transpose((0,2,3,1))) 
    # For later processing, ensure that X has 4 dimensions (add an additional last axis of
    # size 1 if there are fewer dimensions)
    if(len(X.shape)<=3):
        X = X[...,numpy.newaxis]
    # Local centering by subtracting the mean
    X = X-numpy.reshape(numpy.mean(X, axis=(1,2)),(-1,1,1,X.shape[-1])) 
    # Local scaling by dividing by the standard deviation 
    X = X/(numpy.reshape(numpy.std(X, axis=(1,2)),(-1,1,1,X.shape[-1])) + 1e-10) 
    # Transpose to Shape: (Number of samples, Number of Filters, Height, Width)
    X = X.transpose((0,3,1,2)) 
    # Number of batches determined by number of samples in X and batch size
    n_batches = int(numpy.ceil(len(X)/float(batch_size)))
    # Array to store patches modified by recursive autoconvolution
    autoconv_patches = []
    for batch in range(n_batches):
        # Obtain the samples corresponding to the current batch 
        X_order = X[numpy.arange(batch*batch_size, min(len(X)-1,(batch+1)*batch_size))]
        # conv_orders is an array containing the desired orders of recursive autoconvolution
        # (with 0 corresponding to no recursive autoconvolution and 3 corresponding to 
        # recursive autoconvolution of order 3) 
        for conv_order in conv_orders:
            if conv_order > 0:
                # Perform recursive autoconvolution using "autoconv2d"
                X_order = autoconv2d(X_order)
                # In order to perform recursive autoconvolution, the height and width 
                # dimensions of X were doubled. Therefore, after recursive autoconvolution is
                # performed, reduce the height and width dimensions of X by 2. 
                X_sampled = resize_batch(X_order, [int(numpy.round(s/2.)) for s in X_order.shape[2:]])
                if conv_order > 1:
                    X_order = X_sampled
                # Resize X_sampled to the expected shape for MiniBatchKMeans
                if X_sampled.shape[2] != X.shape[2]:
                    X_sampled = resize_batch(X_sampled, X.shape[2:])
            else:
                X_sampled = X_order
            # Append the output of each order of recursive autoconvolution to "autoconv_patches"
            # in order to provide MiniBatchKMeans with a richer set of patches 
            autoconv_patches.append(X_sampled)
        print('%d/%d ' % (batch,n_batches))
    X = numpy.concatenate(autoconv_patches) 
    # Reshape X into a 2-D array for input into MiniBatchKMeans
    X = numpy.asarray(X.reshape(X.shape[0],-1),dtype=numpy.float32)
    # Convert X into an intensity matrix with values ranging from 0 to 1
    X = mat2gray(X)
    # Use PCA to reduce dimensionality of X
    pca = PCA(whiten=True)
    X = pca.fit_transform(X)
    # Scale input sample vectors individually to unit norm (using sklearn's "normalize")
    X = normalize(X)
    # Use "MiniBatchKMeans" on the extracted patches to find a dictionary of n_clusters 
    # filters (centroids)
    km = MiniBatchKMeans(n_clusters = n_clusters,batch_size=batch_size,init_size=3*n_clusters).fit(X).cluster_centers_
    # Reshape centroids into shape: (Number of samples, Number of filters, Height, Width)
    return km.reshape(-1,sz[1],patch_size[0],patch_size[1])

def mat2gray(X):
    ''' mat2gray Method 
            Convert the input X into an intensity image with values ranging from 0 to 1
            
            Args:
                X: (Number of samples, Number of filters, Height, Width)         
            Returns: 
                X: 2-D intensity image matrix
                
    '''
    # Input Shape: (Number of samples, Number of features)
    # Find the minima of X along axis=1, i.e. the minimum value feature for each sample
    m = numpy.min(X,axis=1,keepdims=True)
    # Find the maxima of X along axis=1 and subtract the minima of X from it to obtain the
    # range of values of X for each sample
    X_range = numpy.max(X,axis=1,keepdims=True)-m
    # Set the values of X so that the maximum corresponds to 1, the minimum corresponds to 0,
    # and values larger than the maximum and smaller than the minimum are set to 1 and 0,
    # respectively
    # For samples where the maximum of the features is equal to the minimum of the features
    # set the feature values to 0
    idx = numpy.squeeze(X_range==0)
    X[idx,:] = 0
    # For samples other than those indexed by idx, subtract the minima of the feature values 
    # from the feature values and divide by X_range
    X[numpy.logical_not(idx),:] = (X[numpy.logical_not(idx),:]-m[numpy.logical_not(idx)])/X_range[numpy.logical_not(idx)]
    return X

def learn_connections_unsup(feature_maps,num_groups,group_size,flag):
    ''' learn_connections_unsup Method 
            Learn which groups of feature maps are most strongly correlated (filter 
            connections), or which feature map elements are most strongly correlated (mask
            connections), depending on the flag

            Args:
                feature_maps: (Number of samples, Number of filters, Height, Width)
                num_groups: int number of groups of feature maps (or feature map elements)
                            to learn
                group_size: int size of groups of feature maps (or feature map elements) to 
                            learn
                flag: int (either 0 or 1), with 0 indicating learning feature map (filter)
                      connections and 1 indicating learning feature map element (mask)
                      connections
            Returns: 
                connections: 2-D numpy array, with "num_groups" rows and "group_size" 
                             columns, where values in each row (each group) indicate 
                             feature maps (or feature map elements) that are strongly 
                             correlated
                
    '''
    # Number of samples
    num_samples = feature_maps.shape[0]
    # Number of filters
    num_filters = feature_maps.shape[1]
    # Height
    height = feature_maps.shape[2]
    # Width
    width = feature_maps.shape[3]
    if flag == 0: 
        # A flag of 0 indicates learning filter connections
        # In this function, which is an implementation of MATLAB's pdist2, the pairwise 
        # distance between samples (rows) is determined using the features (columns) of 
        # each sample. For learning filter connections, reshape the input into a 2-D array 
        # where the rows (samples) are the feature maps and the columns (features) are the 
        # feature map elements. Note that the pairwise distances are computed between
        # the feature maps of all the examples. (For this function, "example" means the 
        # patch under consideration and "sample" means the observation, either feature map
        # or feature map element, that is being compared with all the other observations in 
        # the patch). Feature map 1 of example 1, for example, is compared with all the other 
        # feature maps (samples) of example 1, in addition to the feature maps of all the other 
        # examples. This set-up is not ideal (ideal being comparing the feature maps within one 
        # example separately), but prevents a longer set of computations. 
        # The following line reshapes the feature maps so that the rows are the feature
        # maps and the columns are the feature map elements
        feature_maps = feature_maps.reshape((num_samples*num_filters,height*width))
    else: 
        # A flag of 1 indicates learning mask connections
        # The mask is multiplied by the designated layer in the Keras model (output of 
        # maxpooling) to zero out feature map elements that are not strongly correlated with 
        # other elements, the goal being to obtain more discriminative features. In this 
        # case, reshape the input into a 2-D array where the rows (samples) are the feature
        # map elements and the columns (features) are the feature maps. Once again, the 
        # pairwise distances are computed between feature map elements of all examples.
        # The following lines reshape the feature maps so that the rows are the feature
        # map elements and the columns are the. feature maps 
        feature_maps = feature_maps.reshape((num_samples,num_filters,height*width))
        feature_maps = feature_maps.transpose((0,2,1))
        feature_maps = feature_maps.reshape((num_samples*height*width,num_filters))
    # Ensure that the feature maps have values between 0 and 1
    min_max_scaler = MinMaxScaler()
    feature_maps = min_max_scaler.fit_transform(feature_maps)
    # PCA whiten the feature maps
    pca = PCA(.99,whiten=True)
    feature_maps = pca.fit_transform(feature_maps)
    # Square "feature maps" to obtain squared energy 
    feature_maps = numpy.square(feature_maps)
    # Calculate the pairwise distances between samples using sklearn's "pairwise_distances."
    # Use the "mat2gray" function to conver the result into an intensity image with 
    # values ranging from 0 to 1. The result is a square matrix where the number of rows and
    # columns is equal to the number of samples. Every ith row corresponds to a particular 
    # sample and every jth column corresponds to a particular sample. Each (i,j) element
    # therefore represents the correlation between the ith sample and jth sample. 
    S_all = mat2gray(pairwise_distances(feature_maps,feature_maps,metric='correlation'))
    # In the following section of code, the procedure is as follows: a random sample (a 
    # random row) is selected from the set of all samples. The values of all the elements
    # in the row are sorted and the "group_size" highest values are kept. A group is created 
    # using the indexes of these "group_size" highest values. These indexes indicate which 
    # samples in the set of all samples are most closely correlated with the random sample 
    # currently under consideration (the sample corresponding to the random row that was 
    # selected initially). Note, however, that the indexes will indicate which samples in 
    # which examples are most closely correlated with the sample under consideratio. (This is  
    # because the rows of the matrix are organized such that the samples of each example are 
    # concatenated one after another). It is not important to know information about which
    # example a sample came from, only what the sample is itself. The modulo operation is 
    # therefore used, knowing the number of samples in each example, to isolate the identities
    # of the samples and remove information about the particular examples they came from. More
    # random samples are selected until "num_groups" samples are chosen and this procedure 
    # above repeated for each one. The entire procedure involving "num_groups" samples 
    # represents one iteration 
    # Number of iterations to perform the procedure above (i.e. random selection)
    n_it = 100
    # Instantiate a dictionary to contain all the groups 
    connections_all = dict()
    # Instantiate an array "n_it" elements long called "S_total." For each iteration, the 
    # total sum of distances in the group of most strongly correlated features wil be 
    # computed and stored in the element of S_total corresponding to the current iteration.
    S_total = numpy.zeros(n_it)
    for it in range(n_it):
        S_all_it = S_all;
        # Randomly select "num_groups" samples 
        rand_feats = numpy.random.randint(0,S_all_it.shape[0],size=num_groups)
        # Instantiate a temnporary array to hold the groups created in this iteration
        # The array will have "num_groups" rows and "group_size" columns
        connections_tmp = numpy.zeros((num_groups,group_size))
        for ii in range(num_groups):
            while(True):
                S_tmp = S_all_it
                # Prevent selection of the same sample by setting the (i,i) element to 0.
                # For example, suppose there are 10 samples and one example. Sample 3 is 
                # randomly selected. There will be 10 rows and 10 columns after the 
                # "pairwise_distances" function is executed. The (3,3) element value will
                # most likely be high, since a sample would most likely be strongly correlated
                # with itself. Set the (3,3) element to 0 to prevent the same sample from 
                # being selected.
                S_tmp[rand_feats[ii],rand_feats[ii]] = float('inf')
                # Sort the row under consideration to find the most strongly correlated 
                # samples 
                ids = numpy.argsort(S_tmp[rand_feats[ii],:])
                S_min = S_tmp[rand_feats[ii],:][ids]
                # Find the "group_size" most strongly correlated samples 
                closest = S_min[0:group_size-1]
                # The modulo operation depends on the number of samples per example
                if flag == 0:
                    # For learning filter connections, there are "num_filters" samples
                    # per example, so use the modulo operation with "num_filters" to 
                    # identify the identities of the samples 
                    group = numpy.sort(numpy.append([rand_feats[ii]%num_filters],[ids[0:group_size-1]%num_filters]))
                else:
                    # For learning mask connections, there are (height*width) samples per 
                    # example, so use the modulo operation with (height*width) to identify 
                    # the identities of the samples 
                    group = numpy.sort(numpy.append([rand_feats[ii]%(height*width)],[ids[0:group_size-1]%(height*width)]))
                # Only break the while loop if the number of unique elements is equal to 
                # the length of the group, i.e. if all the elements of the group are unique.
                # This encourages selection of groups that connect more samples 
                if(len(numpy.unique(group)) == len(group)):
                    connections_tmp[ii,:] = group
                    break
                # If not all the members of the group are unique, select a new random
                # sample and perform the procedure again
                rand_feats[ii] = numpy.random.randint(0,S_all_it.shape[0],size=1)
            S_all_it = S_tmp
            # Update the element of S_total corresponding to the current iteration with the
            # total sum of distances in the new group 
            S_total[it] = S_total[it]+numpy.sum(closest)
        connections_tmp = numpy.asarray(connections_tmp,dtype=numpy.int32)
        # Store the matrix of "num_groups" groups of size "group_size" for the current 
        # iteration in the corresponding entry of the connections_all matrix
        connections_all[it] = connections_tmp
    # Sort S_total to determine which iterations contained groups with the lowest sum of
    # pairwise distances 
    ids = numpy.argsort(S_total)
    # Instantiate an array that will hold the number of unique samples in a particular 
    # iteration. For sake of efficiency, only examine n_it/10 iterations, not all n_it.
    n_unique_feats = numpy.zeros(round(n_it/10))
    for it in range(round(n_it/10)):
        # Select n_it/10 of the elements in sorted S_total, i.e. the n_it/10 iterations 
        # with the lowest sum of pairwise distances. For each of these elements, find the 
        # number of unique samples in the groups of the iteration under consideration from 
        # connections_all. 
        n_unique_feats[it] = len(numpy.unique(connections_all[ids[it]].flatten()))
    # Locate the member of n_unique_feats corresponding to the greatest number of unique
    # samples. 
    num_maxind = len(numpy.where(n_unique_feats == numpy.amax(n_unique_feats))[0])
    # If there is only one iteration where the value of n_unique_feats is equal to the 
    # greatest number of unique samples, select that iteration. 
    if num_maxind == 1:
        it = ids[numpy.squeeze(numpy.where(n_unique_feats == numpy.amax(n_unique_feats)))]
    # If there are multiple iterations where the value of n_unique_feats is equal to the 
    # greatest number of unique samples, select the first of these iterations.
    else: 
        it = ids[numpy.squeeze(numpy.where(n_unique_feats == numpy.amax(n_unique_feats)))[0]]
    # Find all the unique groups in the chosen iteration 
    connections = numpy.unique(connections_all[it],axis=0)
    # If the function is being used to learn filter connections, perform the following: 
    if flag == 0:
        # If not all the groups in connections_all[it] are unique, this is an issue because 
        # the Keras model expects num_groups groups. Therefore, create groups of randomly
        # selected samples to add to "connections" until the total number of unique groups 
        # is equal to "num_groups."
        while(connections.shape[0] != num_groups):
            indices = numpy.arange(num_filters)
            numpy.random.shuffle(indices)
            connections = numpy.concatenate((connections,indices[0:group_size].reshape((1,group_size))),axis=0)
    return connections

def model_fp(model,model_train,layer_name):
    ''' model_fp Method 
            Create an intermediate version of the input Keras model, where the output is 
            derived from an earlier layer specified by "layer_name"

            Args:
                model: Compiled Keras model 
                model_train: 4-D array of inputs to pass through the new intermediate model 
                             and generate the desired output from an earlier layer.
                             model_train is of shape: 
                             (Number of smaples, height, width, number of filters)
                layer_name: string name of the layer to produce the ouput from in the Keras
                            model
            Returns: 
                output: 4-D array of outputs from the layer specified by "layer_name"
                        output is of shape:
                        (Number of samples, height, width, number of filters)
                
    '''
    # Create an intermediate version of the input model, where the intput is the same, but 
    # the output is derived from a layer preceding the input model's output. ("layer_name"
    # specifies which layer should be the intermediate model's output)
    intermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer(layer_name).output)  
    # Determine the output of the intermediate model when given an input of model_train
    output = intermediate_layer_model.predict(model_train)
    return output 

def autoconv2d(X):
    ''' autoconv2d Method 
            Perform autoconvolution of the input X 

            Args:
                X: (Number of samples, number of filters, height, width)
            Returns: 
                4-D array representing the autoconvolution of X of shape:
                (Number of samples, number of filters, height, width)
                
    '''
    sz = X.shape 
    # Local centering by subtracting the mean
    X -= numpy.reshape(numpy.mean(X, axis=(2,3)),(-1,sz[-3],1,1)) 
    # Local scaling by dividing by the standard deviation 
    X /= numpy.reshape(numpy.std(X, axis=(2,3)),(-1,sz[1],1,1)) + 1e-10 
    # In order to perform autoconvolution, the height and width dimensions of X 
    # need to be doubled. Achieve this using numpy.pad
    X = numpy.pad(X, ((0,0),(0,0),(0,sz[-2]-1),(0,sz[-1]-1)), 'constant') 
    # Return the autoconvolution of X
    return numpy.real(numpy.fft.ifft2(numpy.fft.fft2(X)**2))

def resize_batch(X, new_size):
    ''' resize_batch Method 
            Resize X into desired shape, specified by "new_size"

            Args:
                X: (Number of samples, number of filters, height, width)
            Returns: 
                4-D reshaped array of X 
                
    '''
    out = []
    # Resize X into the desired shape "new_size"
    for x in X:
        out.append(resize(x.transpose((1,2,0)), new_size, order=1, preserve_range=True,mode='constant').transpose((2,0,1)))
    return numpy.stack(out)

def separate_trainlabels(Y_train):
     ''' separate_trainlabels Method 
            Randomly shuffle the indices of the training set and extract arrays "Y_pos"
            and "Y_neg" holding indices of solely positive and solely negative labels, 
            respectively 

            Args:
                Y_train: Labels for the training set X_train of shape: (Number of samples),
                         with labels being either 0 or 1
            Returns: 
                Y_pos: 1-D numpy array of indices to positive labels 
                Y_neg: 1-D numpy array of indices to negative labels
                indices: 1-D array of shuffled indices into the original array Y_train
                
    '''
    # Randomly shuffle the indices of Y_train
    indices = numpy.arange(len(Y_train))
    numpy.random.shuffle(indices)
    # Reorder Y_train according to the shuffled indices 
    labels = Y_train[indices]
    # Initialize arrays "Y_pos" and "Y_neg" to hold the indices of the positive examples 
    # and negative examples, respectively. 
    Y_pos = []
    Y_neg = []
    for ii in range(len(labels)):
        # Add the index of a positive label to Y_pos if the length of Y_pos is less than 50.
        # This ensures Y_pos eventually has a total of 50 positive labels. 
        if labels[ii] == 1:
            if len(Y_pos) < 225:
                Y_pos.append(ii)
        # Add the index of a negative label to Y_neg if the length of Y_neg is less than 50.
        # This ensures Y_neg eventually has a total of. 50 negative labels.
        else:
            if len(Y_neg) < 225:
                Y_neg.append(ii)
        # Break the for loop as soon as Y_pos and Y_neg both have 50 elements 
        if len(Y_pos) == 225 and len(Y_neg) == 225:
            break
    return Y_pos,Y_neg,indices

class roc_callback(Callback):
    ''' roc_callback Class
        Uses keras.callbacks.Callback abstract base class to build new callbacks 
        for visualization of internal states/statistics of the CNN model 
        during training. In this case, print the roc_auc score (from sklearn)
        for every epoch during training 
    '''
    def __init__(self,training_data):
        ''' __init__ Method 
            
            Args:
                training_data: 2 element tuple, the first element of which is the 
                training dataset and the second element of which is the training
                labels  
        '''  
        super(roc_callback,self).__init__()
        self.x = training_data[0]
        self.y = training_data[1]
    def on_train_begin(self, logs={}):
        ''' on_train_begin Method 
            
            Args:
                logs: Dictionary containing keys for quantities relevant 
                to current epoch 
        '''  
        # Add the metric "roc_auc_val" if it does not already exist 
        if not('roc_auc_val' in self.params['metrics']):
            self.params['metrics'].append('roc_auc_val')
    # The following two methods are not necessary for calculating the roc_auc 
    # score. Threfore, simply return 
    def on_train_end(self, logs={}):
        return
    def on_epoch_begin(self, epoch, logs={}):
        return
    def on_epoch_end(self, epoch, logs={}):
        ''' on_epoch_end Method 
            
            Args:
                epoch: Current epoch number  
                logs: Dictionary containing keys for quantities relevant 
                to current epoch 
        '''  
        # Initialize the value of "roc_auc_val" to -inf so that the first calculated value 
        # registers as an improvement in the value of "roc_auc_val" for the EarlyStopping
        # Callback
        logs['roc_auc_val'] = float('-inf')
        y_pred = self.model.predict(self.x)
        roc = roc_auc_score(self.y, y_pred)
        logs['roc_auc_val'] = roc
        print('\rroc-auc: %s' % (str(round(roc,4))),end=100*' '+'\n')
        return
    # The following two methods are not necessary for calculating the roc_auc 
    # score. Threfore, simply return 
    def on_batch_begin(self, batch, logs={}):
        return
    def on_batch_end(self, batch, logs={}):
        return

# Parameters for Training Model:
# Size of Filters (kernel_size x kernel_size) in Keras model for all convolutional layers
kernel_size= 7
# Number of groups of filters 
num_groups_f = 64
# Number of feature maps in each group
group_size_f = 4
# Size of layer 1 output in model after maxpooling
maxpool1_shape = int((X_train.shape[2]-kernel_size+1)/2)
# Size of layer 2 output in model after maxpooling
maxpool2_shape = int((maxpool1_shape-kernel_size+1)/2)
# Number of groups for "learn_connections_unsup" when learning the mask for layer 1
num_groups_m1 = 50
# Number of groups for "learn_connections_unsup" when learning the mask for layer 2
num_groups_m2 = 15
# The size of the group when learning the mask for layer 1
group_size_m1 = maxpool1_shape
# The size of the group when learning the mask for layer 2
group_size_m2 = 5
# Input Shape: (Number of Samples, Height, Width, Number of Filters)
input_shape = (X_train.shape[1],X_train.shape[2],X_train.shape[3])
# Instantiate arrays for connections matrices as the expected sizes so that the Keras
# model can be instantiated with the proper architecture. These values will be changed
# once the connections are learned, and the model updated with these learned values
connections_1f = numpy.arange(num_groups_f*group_size_f).reshape(num_groups_f,group_size_f)
connections_1f = numpy.asarray(connections_1f,numpy.int32)
connections_2f = numpy.arange(num_groups_f*group_size_f).reshape(num_groups_f,group_size_f)
connections_2f = numpy.asarray(connections_2f,numpy.int32)
connections_1m = numpy.ones((1,maxpool1_shape,maxpool1_shape,1))
connections_2m = numpy.ones((1,maxpool2_shape,maxpool2_shape,1))
# Instantiate the Keras model with the proper architecture
model = create_model(num_groups_f,group_size_f,kernel_size,input_shape,connections_1f,connections_2f,connections_1m,connections_2m)
# "k_train" is the set of samples fed to K-Means to learn filters unsupervised
k_train = X_train.transpose((0,3,1,2))
# The "separate_trainlabels" function shuffles the training set (and training labels), and 
# and yields the indices to replicate the shuffling operation. The function also generates
# "Y_pos" and "Y_neg," arrays containing indices of the samples in the training set belonging
# to the positive and negative class, respectively. ("Y_pos" and "Y_neg" are each set to 
# contain 225 samples). Equal number of samples from each of these arrays are therefore given 
# to K-Means, so that K-Means is able to learn its dictionary of filters based off features 
# from both the positive and negative class. 
Y_pos,Y_neg,indices = separate_trainlabels(Y_train)
# Shuffle k_train, model_train, and model_labels according to the indices specified by 
# "separate_trainlabels"
k_train = k_train[indices]
model_train = X_train[indices]
model_labels = Y_train[indices]
# Feed 50 examples from the positive class and 50 examples from the negative class to 
# K-Means to learn a dictionary of filters. Identify those examples in k_train using the
# indices in the Y_pos and Y_neg arrays 
k_train = numpy.concatenate((k_train[Y_pos[0:50]],k_train[Y_neg[0:50]]),axis=0)
# Use the "MiniBatchKMeansAutoConv" function to learn the dictionary of filters. Extract
# 1/3 of the total number of patches randomly from each sample for training, learn
# num_groups_f*group_size_f filters (centroids), and do not use recursive autoconvolution 
centroids_0 = MiniBatchKMeansAutoConv(k_train,(kernel_size,kernel_size),0.33,num_groups_f*group_size_f,[0])
print('finished learning filters')
# The learned filters (centroids) will be set as the weights of the "conv0" layer.
layer_toset = model.get_layer('conv0')
# Reshape the learned filters so that they are the same shape as expected by the Keras layer
filters = centroids_0.transpose((2,3,1,0))
filters = filters[numpy.newaxis,...]
layer_toset.set_weights(filters)
print('finished setting filters')
# Construct an intermediate model (using model_fp) that generates the output of layer 
# "maxpool0." This is the output that the next set of learned filters will be convolved with.
# Therefore, generate the output for further processing (i.e. learning filter and mask 
# connections)
# For sake of efficiency, continue using only the 225 samples in Y_pos and 225 samples in Y_neg 
# for the processing below (i.e. forward pass only these 450 samples from the training set
# through the Keras model and obtain the output of layer "maxpool0" for only these samples).
fp_train = numpy.concatenate((model_train[Y_pos],model_train[Y_neg]),axis=0)
output_0 = model_fp(model,fp_train,'maxpool0')
# Transpose the output into shape: (Number of samples, number of filters, height, width)
# for "learn_connections_unsup" and "MiniBatchKMeansAutoConv"
output_0 = output_0.transpose((0,3,1,2))
print('finished producing output')
# For sake of efficiency, use only the first 10 and last 10 examples from output_0 for
# learning filter connections via "learn_connections_unsup"
output_0_learnf = numpy.concatenate((output_0[0:10],output_0[-10:]),axis=0)
print('learning connections')
# Use the "learn_connections_unsup" function to find "num_groups_f" groups of feature maps 
# of size "group_size" that are most strongly correlated (using the "energy correlation" 
# pairwise similarity metric). In this case, to calculate the pairwise similaritiy metrics,
# the "samples" are the feature maps (depth dimension) and the "features" are the elements in 
# each feature map (height*width dimension) 
connections_1f = learn_connections_unsup(output_0_learnf,num_groups_f,group_size_f,flag=0)
print('finished learning f connections')
# Use the "learn_connections_unsup" function to find the "num_groups_m1" groups of feature
# map elements of size "group_size_m1" that are most strongly correlated. In this case, to 
# calculate the pairwise similarity metrics, the "samples" are the feature map elements
# (height*width dimension) and the "features" are the different feature maps (depth dimension)
# For sake of efficiency, again use only the first 10 and last 10 samples of output_0 
output_0_learnm = numpy.concatenate((output_0[0:10],output_0[-10:]),axis=0)
mask_1m = learn_connections_unsup(output_0_learnm,num_groups_m1,group_size_m1,flag=1)
print('finished learning m connections')
# Find all the unique feature map elements selected using "learn_connections_unsup
mask_1m = numpy.unique(mask_1m.flatten())
# Create a mask by setting all the selected feature map elements to 1, and the non-selected
# feature map elements to 0. This ensures that the feature map elements not strongly 
# correlated with others are zeroed out during training the Keras model. Note that the mask 
# is set up for the elements in one feature map, and broadcasted to all the other feature
# maps. (Therefore, if the top leftmost element were zeroed out in the mask, it would be 
# zeroed out in every feature map). Also note that learn_connections_unsup generates indices 
# of the correlated feature map elements as if the one feature map to be broadcasted were 
# flattened. Therefore, the indices are set on a flattened version of the feature map
# to be broadcasted, then reshaped into the proper shape.
connections_1m = numpy.zeros((output_0.shape[2]*output_0.shape[3]))
connections_1m[mask_1m] = 1
connections_1m = connections_1m.reshape((output_0.shape[2],output_0.shape[3]))  
connections_1m = connections_1m[numpy.newaxis,numpy.newaxis,:,:]
print('finished making connections_1m')
# The feature maps fed into K-Means to learn the next set of filters are split into groups of 
# reduced dimensionality (to boost performance), with the groups determined by the results
# of connections_1f. Achieve this splitting by repeating output_0 "num_groups_f" times, each
# time using only the filters in one of the "num_groups_f" groups of connections_1f. 
# "output_0_connected" holds all these versions of output_0 with the different sets of 
# filters selected. K-Means will receive samples from each of these different versions of 
# output_0 to learn the next dictionary of filters 
output_0_connected = numpy.zeros((output_0.shape[0]*num_groups_f,group_size_f,output_0.shape[2],output_0.shape[3]))
index = 0
# For sake of efficiency, do not use all the samples in each version of output_0. Only use
# the first and last 10 samples in each version of output_0. "k_train_indices" keeps track
# of what all these indices are, so that the appropriate samples may be selected for feeding
# into K-Means
k_train_indices = []
for ii in range(num_groups_f):
    output_0_connected[index:index+output_0.shape[0],:,:,:] = output_0[:,connections_1f[ii,:],:,:]
    temp1 = numpy.arange(index,index+10)
    k_train_indices.extend(temp1)
    temp2 = numpy.arange((index+output_0.shape[0])-10,(index+output_0.shape[0]))
    k_train_indices.extend(temp2)
    index = index + output_0.shape[0]
print('finished output_0_connected')
k_train = output_0_connected[k_train_indices]
print('learning filters for layer 1')
# Use the "MiniBatchKMeansAutoConv" function to learn the dictionary of filters. Extract
# 1/3 of the total number of patches randomly from each sample for training, learn
# group_size_f filters (centroids), and do not use recursive autoconvolution 
# Since group_size_f filters are learned, all the filters convolved with each of
# the "num_groups_f" groups in the Keras model, and the results concatenated, there will be
# a total of num_groups_f*group_size_f filters in the next layer.
centroids_1 = MiniBatchKMeansAutoConv(k_train,(kernel_size,kernel_size),0.33,group_size_f,[0])
print('finished learning filters')
# Reshape the mask into the shape expected by the Keras model
connections_1m = connections_1m.transpose((0,2,3,1))
# Re-instantiate the model with the newly-set connections_1f and connections_1m 
model = create_model(num_groups_f,group_size_f,kernel_size,input_shape,connections_1f,connections_2f,connections_1m,connections_2m)
print('finished creating model')
# Since the model is re-instantiated, re-set the filters for "conv_0" 
layer_toset = model.get_layer('conv0')
filters = centroids_0.transpose((2,3,1,0))
filters = filters[numpy.newaxis,...]
layer_toset.set_weights(filters)
print('finished setting filters')
# Since the the new set of learned filters is convolved with each of the smaller groups of
# feature maps of reduced dimensionality, each learned filter is the same shape (height, width,
# depth) as each of these smaller groups. Therefore, there is one "conv" layer for each of 
# these smaller groups in the Keras model that will be set to the newly-learned set of filters.
# Set those filters using the for loop below 
for ii in range(num_groups_f):
    layer_toset = model.get_layer('conv1_'+str(ii))
    filters = centroids_1.transpose((2,3,1,0))
    filters = filters[numpy.newaxis,...]
    layer_toset.set_weights(filters)
print('finished setting filters')
# For sake of efficiency, continue using only the 225 samples in Y_pos and 225 samples in Y_neg 
# for the processing below (i.e. forward pass only these 450 samples from the training set
# through the Keras model and obtain the output of layer "final1" for only these samples).
output_1 = model_fp(model,fp_train,'final1')
# Transpose output_1 for "learn_connections_unsup" and "MiniBatchKMeansAutoConv"
output_1 = output_1.transpose((0,3,1,2))
print('finished producing output')
# For sake of efficiency, use only the first 10 and last 10 samples of output_1 for 
# "learn_connections_unsup"
output_1_learnf = numpy.concatenate((output_1[0:10],output_1[-10:]),axis=0)
print('learning connections')
# Learn filter connections
connections_2f = learn_connections_unsup(output_1_learnf,num_groups_f,group_size_f,flag=0)
print('finished learning f connections')
# Learn mask connections
output_1_learnm = numpy.concatenate((output_1[0:10],output_1[-10:]),axis=0)
mask_2m = learn_connections_unsup(output_1_learnm,num_groups_m2,group_size_m2,flag=1)
print('finished learning m connections')
# Apply the mask to the representative feature map (to be broadcasted to all feature maps)
mask_2m = numpy.unique(mask_2m.flatten())
connections_2m = numpy.zeros((output_1.shape[2]*output_1.shape[3]))
connections_2m[mask_2m] = 1
connections_2m = connections_2m.reshape((output_1.shape[2],output_1.shape[3])) 
connections_2m = connections_2m[numpy.newaxis,numpy.newaxis,:,:]
print('finished making connections_2m')
# Repeat output_1 "num_groups_f" times, each time using only the filters in one of the 
# "num_groups_f" groups of connections_2f. "output_1_connected" holds all these versions of 
# output_1 with the different sets of filters selected. 
output_1_connected = numpy.zeros((output_1.shape[0]*num_groups_f,group_size_f,output_1.shape[2],output_1.shape[3]))
index = 0
for ii in range(num_groups_f):
    output_1_connected[index:index+output_1.shape[0],:,:,:] = output_1[:,connections_2f[ii,:],:,:]
    index = index + output_1.shape[0]
print('finished output_1_connected')
k_train = output_1_connected
print('learning filters for layer 2')
# Learn the next set of filters using all samples from output_1_connected. Take one half of
# the maximum number of patches per sample and laern group_size_f filters (centroids).
# Since group_size_f filters are learned, all the filters convolved with each of the 
# "num_groups_f" groups in the Keras model, and the results concatenated, 
# there will be a total of num_groups_f*group_size_f filters in the next layer.
centroids_2 = MiniBatchKMeansAutoConv(k_train,(kernel_size,kernel_size),0.5,group_size_f,[0])
print('finished learning filters')
# Reshape connections_2m into the shape expected by the Keras model
connections_2m = connections_2m.transpose((0,2,3,1))
# Re-instantiate the Keras model with the newly-set connections_2f and connections_2m
model = create_model(num_groups_f,group_size_f,kernel_size,input_shape,connections_1f,connections_2f,connections_1m,connections_2m)
# Since the model is re-instantiated, re-set the filters for "conv_0" 
layer_toset = model.get_layer('conv0')
filters = centroids_0.transpose((2,3,1,0))
filters = filters[numpy.newaxis,...]
layer_toset.set_weights(filters)
print('finished setting filters')
# Since the model is re-instantiated, re-set the filters for convolutional layers in Layer 1
for ii in range(num_groups_f):
    layer_toset = model.get_layer('conv1_'+str(ii))
    filters = centroids_1.transpose((2,3,1,0))
    filters = filters[numpy.newaxis,...]
    layer_toset.set_weights(filters)
print('finished setting filters')
# Set the filters for convolutional layers in Layer 2
for ii in range(num_groups_f):
    layer_toset = model.get_layer('conv2_'+str(ii))
    filters = centroids_2.transpose((2,3,1,0))
    filters = filters[numpy.newaxis,...]
    layer_toset.set_weights(filters)
